{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Reinforcement Learning - Q-Network\n\nIn this notebook a vanilla Q-Network is implemented with Tensorflow 2.3 and trained to complete the CartPole-v0 environment of the OpenAI gym.\nThe **focus** of this implementation is to get the algorithm to work and provide a starting point for further tweaks and experiments.\nTherefore: Simple architecture, simple task, no fancy extras.\n\n## This includes:\n- Experience Replay / Replay Buffer\n- An epsilon schedule\n- The E-Greedy Policy\n- The compute graph of the Deep Q-Network\n- Weight synchronization between Q-Network and Target-Network\n- Implementation of the training loop\n\n## References\n- Mnih et al. 2013, [\"Playing Atari with Deep Reinforcement Learning\"](https://arxiv.org/abs/1312.5602)\n- Mnih et al. 2015, [\"Human Level Control Through Deep Reinforcement Learning\"](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)"},{"metadata":{},"cell_type":"markdown","source":"# Dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython import display\nimport time\nimport cv2\nimport tensorflow as tf\nimport random as rand\nfrom collections import deque","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gym[atari] ","execution_count":10,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: gym[atari] in /opt/conda/lib/python3.7/site-packages (0.17.2)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.5.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.4.1)\nRequirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.3.0)\nRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.18.5)\nRequirement already satisfied: opencv-python; extra == \"atari\" in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (4.4.0.42)\nRequirement already satisfied: Pillow; extra == \"atari\" in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (7.2.0)\nRequirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (0.2.6)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.14.0)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Check out the game"},{"metadata":{"trusted":true},"cell_type":"code","source":"GAME = \"CartPole-v0\"\nenv = gym.envs.make(GAME)\nprint(\"Action space: {}\".format(env.action_space))\nprint(\"Action space size: {}\".format(env.action_space.n))\nobservation = env.reset()\nprint(\"Observation space shape: {}\".format(observation.shape))\n\nprint(\"-\"*10)\naction = env.action_space.sample()\nprint('Take action {}'.format(action))\nobservation, reward, game_over, info = env.step(action)\nprint(\"observation: {}, reward: {}, game_over: {}, info: {} \".format(observation.shape, reward, game_over, info))\n\nenv.close()","execution_count":11,"outputs":[{"output_type":"stream","text":"Action space: Discrete(2)\nAction space size: 2\nObservation space shape: (4,)\n----------\nTake action 1\nobservation: (4,), reward: 1.0, game_over: False, info: {} \n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Experience Replay\n\nThe **replay buffer** stores the last $N$ state transitions as tuples $(S, A, R, S')$ in a ring buffer.\nThis way collected experiences can be reused and the correlation between samples is broken by sampling random minibatches of the buffer."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, buffer_size):\n        self.buffer_size = buffer_size\n        # deque is actually implemented as linked list, so this is a suboptimal solution for random sampling. A custom ring buffer would be better.\n        # However, for education purposes this will suffice.\n        self.replay_memory = deque(maxlen=buffer_size)    \n\n    def add(self, state, action, reward, next_state, done):\n        self.replay_memory.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        if batch_size <= len(self.replay_memory):\n            return rand.sample(self.replay_memory, batch_size)\n        else:\n            assert False\n\n    def __len__(self):\n        return len(self.replay_memory)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Epsilon Schedule\n\nAn epsilon schedule gives more fine grained control over the way the e-greedy policy unfolds.\nIn this case, a schedule is implemented that linearly interpolates between a start and end epsilon value.\nAdditionally, the schedule can be delayed for a given amount of steps to emphasize exploration very early during training and fill the replay buffer with samples to start the training."},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearSchedule():\n    def __init__(self, start_epsilon=1, final_epsilon=0.1, pre_train_steps=10, final_exploration_step=100):\n        self.pre_train_steps = pre_train_steps\n        self.final_exploration_step = final_exploration_step\n        self.final_epsilon = final_epsilon\n        self.decay_factor = self.pre_train_steps/self.final_exploration_step\n        self.epsilon = self.pre_train_steps * (1-self.decay_factor) + self.final_exploration_step * self.decay_factor\n    \n    def value(self, t):\n        if t > self.pre_train_steps:\n            self.decay_factor = (t - self.pre_train_steps)/self.final_exploration_step\n            self.epsilon = 1-self.decay_factor\n            self.epsilon = max(self.final_epsilon, self.epsilon)\n            return self.epsilon\n        else:\n            return 1","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Implementation\n\nSince states of the CartPole-v0 environment are rather simple (i.e. consisting of 4 float values each), a simple fully-connected neural network is also sufficient."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQN(tf.keras.Model):\n    def __init__(self, input_shape, num_actions):\n        super(DQN, self).__init__()\n        self.input_layer = tf.keras.layers.InputLayer(input_shape=input_shape)\n        self.hidden_layers = []\n        self.hidden_layers.append(tf.keras.layers.Dense(64, activation='relu'))\n        self.hidden_layers.append(tf.keras.layers.Dense(32, activation='relu'))\n        self.output_layer = tf.keras.layers.Dense(units=num_actions, activation='linear')\n\n    @tf.function\n    def call(self, inputs):\n        z = self.input_layer(inputs)\n        for l in self.hidden_layers:\n            z = l(z)\n        q_vals = self.output_layer(z)\n        return q_vals","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Loop\nThe **Agent** class is wrapping all the necessary building blocks and ties their functionality together to train the model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Agent:\n    def __init__(self, epsilon_schedule, replay_buffer, num_actions=2, gamma=0.9, batch_size=64, lr=0.001,\n                 max_episodes = 500, max_steps_per_episode=2000, steps_until_sync=20, choose_action_frequency=1,\n                 pre_train_steps = 1, train_frequency=1):\n        \n        # dqn is used to predict Q-values to decide which action to take\n        self.dqn = DQN([4], num_actions)\n        self.dqn.build(tf.TensorShape([None, 4]))\n        \n        # dqn_target is used to predict the future reward\n        self.dqn_target = DQN([4], num_actions)\n        self.dqn_target.build(tf.TensorShape([None, 4]))\n\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.optimizer = tf.optimizers.Adam(lr)\n        self.gamma = gamma\n        self.replay_buffer = replay_buffer\n        self.epsilon_schedule = epsilon_schedule\n        self.steps_until_sync = steps_until_sync\n        self.choose_action_frequency = choose_action_frequency\n        self.max_episodes = max_episodes\n        self.max_steps_per_episode = max_steps_per_episode\n        self.train_frequency = train_frequency\n        self.loss_function = tf.keras.losses.MSE\n        self.pre_train_steps = pre_train_steps\n        \n        self.episode_reward_history = []\n\n    def predict_q(self, inputs):\n        return self.dqn(inputs)\n\n    def get_action(self, states, epsilon):\n        if np.random.random() < epsilon:\n            # explore\n            return np.random.choice(self.num_actions)\n        else:\n            # exploit\n            return np.argmax(self.predict_q(np.expand_dims(states, axis=0))[0])\n\n    def update_target_network(self):\n        self.dqn_target.set_weights(self.dqn.get_weights())\n\n    def train_step(self):\n        mini_batch = self.replay_buffer.sample(self.batch_size)\n\n        observations_batch, action_batch, reward_batch, next_observations_batch, done_batch = map(np.array,\n                                                                                                  zip(*mini_batch))\n\n        with tf.GradientTape() as tape:\n            dqn_variables = self.dqn.trainable_variables\n            tape.watch(dqn_variables)\n\n            future_rewards = self.dqn_target(tf.convert_to_tensor(next_observations_batch, dtype=tf.float32))\n            next_action = tf.argmax(future_rewards, axis=1)\n            target_q = tf.reduce_sum(tf.one_hot(next_action, self.num_actions) * future_rewards, axis=1)\n            target_q = (1 - done_batch) * self.gamma * target_q + reward_batch\n\n            predicted_q = self.dqn(tf.convert_to_tensor(observations_batch, dtype=tf.float32))\n            predicted_q = tf.reduce_sum(tf.one_hot(action_batch, self.num_actions) * predicted_q, axis=1)\n            loss = self.loss_function(target_q, predicted_q)\n            \n        # Backprop\n        gradients = tape.gradient(loss, dqn_variables)\n        self.optimizer.apply_gradients(zip(gradients, dqn_variables))\n        \n        return loss\n\n    def train(self, env):\n        episode = 0\n        total_step = 0\n        episode_step = 0\n        state = env.reset()\n        loss = 0\n        last_hundred_rewards = deque(maxlen=100)\n\n        while episode < self.max_episodes:\n            current_state = env.reset()\n            done = False\n            action = 0\n            episode_reward = 0\n            episode_step = 0\n            epsilon = epsilon_schedule.value(total_step)\n\n            while not done:\n                if total_step % self.choose_action_frequency == 0:\n                    if len(replay_buffer) > self.batch_size:\n                        action = self.get_action(current_state, epsilon)\n                    else:\n                        action = self.get_action(current_state, 1.0)\n\n                next_state, reward, done, info = env.step(action)\n                \n                self.replay_buffer.add(current_state, action, reward, next_state, done)\n                episode_reward += reward\n\n                if total_step > self.pre_train_steps and len(replay_buffer) > self.batch_size:\n                    loss = self.train_step()\n\n                if total_step % self.steps_until_sync == 0:\n                    self.update_target_network()\n                                    \n                #end of step\n                total_step += 1\n                episode_step += 1\n                current_state = next_state\n                \n            # end of episode\n            self.episode_reward_history.append(episode_reward)\n            last_hundred_rewards.append(episode_reward)\n            mean_episode_reward = np.mean(last_hundred_rewards)\n            \n            if episode % 50 == 0:\n                print(f'Episode {episode} (Step {total_step}) - Moving Avg Reward: {mean_episode_reward:.3f} Loss: {loss:.5f} Epsilon: {epsilon:.3f}')\n\n            if mean_episode_reward >= 195:\n                print(f'Task solved after {episode} episodes! (Moving Avg Reward: {mean_episode_reward:.3f})')\n                return\n                \n            episode += 1\n            \n","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.envs.make(GAME)\n\nepsilon_schedule = LinearSchedule(start_epsilon=1, final_epsilon=0.1, pre_train_steps=100, final_exploration_step=10_000)\n\nreplay_buffer = ReplayBuffer(32_000)\n\nagent = Agent(epsilon_schedule, replay_buffer, num_actions=2, gamma=0.99, batch_size=64, lr=0.0007,\n                 max_episodes=3000, steps_until_sync=200, choose_action_frequency=1)\nagent.train(env)\n\nenv.close()","execution_count":25,"outputs":[{"output_type":"stream","text":"Episode 0 (Step 42) - Moving Avg Reward: 42.000 Loss: 0.00000 Epsilon: 1.000\nEpisode 50 (Step 1249) - Moving Avg Reward: 24.490 Loss: 0.50822 Epsilon: 0.888\nEpisode 100 (Step 3036) - Moving Avg Reward: 29.940 Loss: 0.59516 Epsilon: 0.710\nEpisode 150 (Step 8209) - Moving Avg Reward: 69.600 Loss: 2.29748 Epsilon: 0.209\nEpisode 200 (Step 17925) - Moving Avg Reward: 148.890 Loss: 3.50066 Epsilon: 0.100\nEpisode 250 (Step 27249) - Moving Avg Reward: 190.400 Loss: 2.16448 Epsilon: 0.100\nEpisode 300 (Step 35514) - Moving Avg Reward: 175.890 Loss: 1.00825 Epsilon: 0.100\nEpisode 350 (Step 41598) - Moving Avg Reward: 143.490 Loss: 1.28026 Epsilon: 0.100\nEpisode 400 (Step 51577) - Moving Avg Reward: 160.630 Loss: 1.21683 Epsilon: 0.100\nTask solved after 436 episodes! (Moving Avg Reward: 195.170)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}